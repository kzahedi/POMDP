{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using PyPlot\n",
    "#addprocs(4)\n",
    "@everywhere using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere type Experiment\n",
    "    current_state::Int64\n",
    "    policy::Vector{Float64}\n",
    "    world_state_policies::Matrix{Float64}\n",
    "    nr_world_states::Int64\n",
    "    nr_actions::Int64\n",
    "    target_sequence::Vector{Float64}\n",
    "    sum_of_rewards::Vector{Float64}\n",
    "    nr_of_evaluations::Vector{Float64}\n",
    "    value_function::Vector{Float64} # on the world state\n",
    "    state_action_value_function::Matrix{Float64} # on the world state\n",
    "    k::Int64\n",
    "    value_history::Matrix{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@everywhere function create_experiment(nr_world_states::Int64, nr_actions::Int64, target_actions::Int64, k::Int64)\n",
    "    policy = ones(nr_actions)\n",
    "    policy = policy / sum(policy)\n",
    "    \n",
    "    min = minimum([target_actions, nr_world_states])\n",
    "    max = maximum([0, nr_world_states - nr_actions])\n",
    "    \n",
    "    sequence = vcat([1:min], int(ceil(rand(max) * nr_actions)))\n",
    "    sequence = sequence[randperm(length(sequence))]\n",
    "    \n",
    "    return Experiment(1, policy, zeros(2,2), nr_world_states, nr_actions, sequence, ones(nr_world_states), zeros(nr_world_states), zeros(nr_world_states), zeros(nr_world_states, nr_actions), k, zeros(2,2))\n",
    "end\n",
    "\n",
    "@everywhere function run_mc_episode!(experiment::Experiment, initial_state::Int64, T::Int64)\n",
    "    experiment.current_state = initial_state\n",
    "    for t = 1:T\n",
    "        p = rand()\n",
    "        action = experiment.nr_actions\n",
    "        for i = 1:experiment.nr_actions\n",
    "            if p < sum(experiment.policy[1:i])\n",
    "                action = i\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        if action == experiment.target_sequence[experiment.current_state]\n",
    "            experiment.current_state = experiment.current_state + 1\n",
    "        else\n",
    "            experiment.current_state = 1\n",
    "        end\n",
    "        if experiment.current_state >= experiment.nr_world_states\n",
    "            return 1\n",
    "        end\n",
    "    end\n",
    "    return 0\n",
    "end\n",
    "\n",
    "@everywhere function monte_carlo_estimation_of_world_state_function!(experiment::Experiment, T::Int64, nr_of_episodes::Int64)\n",
    "    for w = 1:experiment.nr_world_states\n",
    "        experiment.sum_of_rewards[w]    = 0.0\n",
    "        experiment.nr_of_evaluations[w] = 0.0\n",
    "    end\n",
    "\n",
    "    for e = 1:nr_of_episodes\n",
    "        initial_state                               = int64(ceil(rand()*experiment.nr_world_states))\n",
    "        reward                                      = run_mc_episode!(experiment, initial_state, T)\n",
    "        experiment.sum_of_rewards[initial_state]    = experiment.sum_of_rewards[initial_state] + reward\n",
    "        experiment.nr_of_evaluations[initial_state] = experiment.nr_of_evaluations[initial_state] + 1.0\n",
    "        experiment.value_function[initial_state]    = experiment.sum_of_rewards[initial_state] /\n",
    "                                                      experiment.nr_of_evaluations[initial_state]\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function run_td_episode!(experiment::Experiment, T::Int64)\n",
    "    experiment.current_state = ceil(rand() * experiment.nr_world_states)\n",
    "    for t = 1:T\n",
    "        action = ceil(rand() * experiment.nr_actions)\n",
    "\n",
    "        old_state = experiment.current_state\n",
    "        new_state = experiment.current_state\n",
    "        \n",
    "        if action == experiment.target_sequence[experiment.current_state]\n",
    "            new_state = new_state + 1\n",
    "        else\n",
    "            new_state = 1\n",
    "        end\n",
    "        \n",
    "        old_value = experiment.value_function[old_state]\n",
    "        new_value = (new_state > experiment.nr_world_states)?experiment.value_function[1]:experiment.value_function[new_state]\n",
    "        reward    = (new_state > experiment.nr_world_states)?1:0\n",
    "        α = 0.5\n",
    "        γ = 0.9\n",
    "        \n",
    "        if new_state > experiment.nr_world_states\n",
    "            experiment.current_state = ceil(rand() * experiment.nr_world_states)\n",
    "        end\n",
    "        \n",
    "        experiment.value_function[old_state] = old_value + α * (reward + γ * new_value - old_value)\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function td_estimation_of_world_state_function!(experiment::Experiment, T::Int64, nr_of_episodes::Int64)\n",
    "    for e = 1:nr_of_episodes\n",
    "        experiment.current_state = ceil(rand() * experiment.nr_world_states)\n",
    "        run_td_episode!(experiment, T)\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function calculate_world_state_action_function!(experiment::Experiment)\n",
    "    for i = 1:experiment.nr_world_states\n",
    "        for action = 1:experiment.nr_actions\n",
    "            experiment.state_action_value_function[i,action] = 0.0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for i = 1:experiment.nr_world_states\n",
    "        for action = 1:experiment.nr_actions\n",
    "            if action == experiment.target_sequence[i]\n",
    "                if i == experiment.nr_world_states\n",
    "                    experiment.state_action_value_function[i,action] = 1.0\n",
    "                else\n",
    "                    experiment.state_action_value_function[i,action] = experiment.value_function[i+1]\n",
    "                end\n",
    "            else\n",
    "                experiment.state_action_value_function[i,action] = 0.0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function prune_world_state_action_function!(experiment::Experiment)\n",
    "    nr_of_values = minimum([experiment.k, experiment.nr_actions])\n",
    "    for i = 1:experiment.nr_world_states\n",
    "        if sum(experiment.state_action_value_function[i,:]) > 0.0\n",
    "            values = DataFrame(VALUES=[v for v in experiment.state_action_value_function[i,:]], INDICES=[1:experiment.nr_actions])\n",
    "            sort!(values, cols = (:VALUES), rev=true)\n",
    "\n",
    "            values = values[1:nr_of_values,:]\n",
    "            for a = 1:experiment.nr_actions\n",
    "                experiment.state_action_value_function[i,a] = 0.0\n",
    "            end\n",
    "\n",
    "            for a = 1:size(values)[1]\n",
    "                experiment.state_action_value_function[i,values[:INDICES][a]] = values[:VALUES][a]\n",
    "            end\n",
    "        else\n",
    "            indices = randperm(experiment.nr_actions)[1:nr_of_values]\n",
    "            for a in indices\n",
    "                experiment.state_action_value_function[i, a] = 0.001\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function update_policy_from_world_state_action_function!(experiment::Experiment)\n",
    "    sum_of_values = zeros(experiment.nr_actions)\n",
    "    for a = 1:experiment.nr_actions\n",
    "        sum_of_values[a] = sum(experiment.state_action_value_function[:,a])\n",
    "    end\n",
    "    \n",
    "    s = sum(sum_of_values)\n",
    "    for a = 1:experiment.nr_actions\n",
    "        experiment.policy[a] = sum_of_values[a] / s\n",
    "    end\n",
    "end\n",
    "\n",
    "@everywhere function update_policy!(experiment::Experiment)\n",
    "    calculate_world_state_action_function!(experiment)\n",
    "    prune_world_state_action_function!(experiment)\n",
    "    update_policy_from_world_state_action_function!(experiment)\n",
    "end\n",
    "\n",
    "@everywhere function scan_over_k(k::Int64, N::Int64, episode_length::Int64, nr_of_episodes::Int64)\n",
    "    nr_world_states   = k\n",
    "    nr_of_actions     = k\n",
    "    experiment        = create_experiment(nr_world_states, nr_of_actions, nr_of_actions, k)\n",
    "    experiment.value_history = zeros(N, nr_world_states)\n",
    "    pm = Progress(N, 1)\n",
    "    for i = 1:N\n",
    "        #        monte_carlo_estimation_of_world_state_function!(exp, episode_length, nr_of_episodes)\n",
    "        td_estimation_of_world_state_function!(experiment, episode_length, nr_of_episodes)        \n",
    "        update_policy!(exp)\n",
    "        experiment.value_history[i,:] = experiment.value_function\n",
    "        next!(pm)\n",
    "    end\n",
    "    return exp\n",
    "end\n",
    "\n",
    "function evaluate(experiment::Experiment, N::Int64)\n",
    "    r = 0.0\n",
    "    for n = 1:N\n",
    "        r = r + run_mc_episode!(experiment, 1, experiment.k + 1)\n",
    "    end\n",
    "    r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00013925831220640592,0.0002846353881672688,0.006702701851759142,0.29645408339326584,0.6643396772658501]\n",
      "[0.3376085266674994,0.15065399485226458,0.00014464789222295505,0.5081866072743958,0.0034062233136172887]\n",
      "[0.0 0.0 0.0002846353881672688 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.006702701851759142\n",
      " 0.0 0.29645408339326584 0.0 0.0 0.0\n",
      " 0.6643396772658501 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 1.0 0.0]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "`Experiment` has no method matching Experiment(::Int64, ::Array{Float64,1}, ::Int64, ::Int64, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,2}, ::Int64, ::Array{Float64,2})\nwhile loading In[5], in expression starting on line 16",
     "output_type": "error",
     "traceback": [
      "`Experiment` has no method matching Experiment(::Int64, ::Array{Float64,1}, ::Int64, ::Int64, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,2}, ::Int64, ::Array{Float64,2})\nwhile loading In[5], in expression starting on line 16",
      ""
     ]
    }
   ],
   "source": [
    "N                 = 100000\n",
    "k                 = 5\n",
    "nr_of_episodes    = 1000\n",
    "nr_world_states   = k\n",
    "nr_of_actions     = k\n",
    "episode_length    = 100\n",
    "experiment        = create_experiment(nr_world_states, nr_of_actions, nr_of_actions, k)\n",
    "experiment.value_history = zeros(N, nr_world_states)\n",
    "\n",
    "td_estimation_of_world_state_function!(experiment, episode_length, nr_of_episodes)\n",
    "update_policy!(experiment)\n",
    "println(experiment.value_function)\n",
    "println(experiment.policy)\n",
    "println(experiment.state_action_value_function)\n",
    "\n",
    "control = Experiment(1, ones(experiment.nr_actions) * 1.0 / float64(experiment.nr_actions), experiment.nr_world_states, experiment.nr_actions, experiment.target_sequence, ones(1), zeros(1), zeros(1), zeros(2, 2), k, zeros(2,2))\n",
    "control = evaluate(control, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy           [0.01,0.01,0.01,0.96,0.01] 1.0000000000000002\n",
      "target sequence: [3.0,5.0,2.0,1.0,4.0]\n",
      "value function:  [0.01,0.01,0.01,0.01,0.01]\n",
      "state action value function [0.0 0.0 0.01 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.01\n",
      " 0.0 0.01 0.0 0.0 0.0\n",
      " 0.01 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 1.0 0.0]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "control not defined\nwhile loading In[6], in expression starting on line 11",
     "output_type": "error",
     "traceback": [
      "control not defined\nwhile loading In[6], in expression starting on line 11",
      ""
     ]
    }
   ],
   "source": [
    "for i = 1:10000\n",
    "    td_estimation_of_world_state_function!(experiment, k, nr_of_episodes)\n",
    "    update_policy!(experiment)\n",
    "end\n",
    "println(\"policy           $(round(experiment.policy,2)) $(sum(experiment.policy))\")\n",
    "println(\"target sequence: $(round(experiment.target_sequence,2))\")\n",
    "println(\"value function:  $(round(experiment.value_function,2))\")\n",
    "println(\"state action value function $(round(experiment.state_action_value_function,2))\")\n",
    "\n",
    "learned = evaluate(experiment, 10000)\n",
    "println(\"learned $learned vs. control $control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.7",
   "language": "julia",
   "name": "julia 0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
